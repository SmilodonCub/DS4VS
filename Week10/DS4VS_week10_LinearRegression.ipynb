{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c98ac55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a061a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "# Week 10: Introduction to Supervised Learning: Numeric Targets\n",
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2321a31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a Brief Recap:\n",
    "\n",
    "* Hello, how are you?\n",
    "* Any questions for homework 3?\n",
    "* Today: Supervised Learning Methods for Numeric Targets\n",
    "    - Multiple Linear Regression\n",
    "    - Ridge & Lasso\n",
    "* Next Week: Supervised Learning Methods for Categorical Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932aec6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Regression\n",
    "\n",
    "Basic Supervised Regression Algorithms:  \n",
    "\n",
    "* Generalized Linear Models\n",
    "* Ridge Regression\n",
    "* Lasso Regression\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ec6a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today we will focus on extending our understanding of Linear Regression and then move on to Ridge and Lasso Regression.  \n",
    "\n",
    "Next week we will work on Decision Trees & Random Forest as well as kNN & SVM. However, it will be for the case where the target variable is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5c1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building on Simple Linear Regression Models\n",
    "\n",
    "Last week we considered simple linear regression where: \n",
    "$$Y \\approx \\beta_0 + \\beta_1X$$\n",
    "\n",
    "But the world is complicated!  \n",
    "It is rare for a single variable to have a strong and consistent linear relationship to a predictor.  \n",
    "\n",
    "Can you name any simple linear systems? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d4ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today we will consider systems where the model describes the relationship of multiple predictor values to a single numeric target variable.  \n",
    "\n",
    "$$Y \\approx \\beta_0 + \\beta_1X + \\beta_2X + \\dots + \\beta_NX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57d1a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Classic Example Dataset\n",
    "\n",
    "NYC Italian Restaurant DataSet.  \n",
    "\n",
    "Let's get our environment set up...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f28cc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as np\n",
    "import matplotlib \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b302b32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/datasets/nyc.csv'\n",
    "df = pd.read_csv( path, encoding= 'unicode_escape' )\n",
    "print( df.head(), '\\n\\n' )\n",
    "print( df.info() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5e4b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# note the similar spread of the predictor varaibles\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70aa584",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our Goal:\n",
    "\n",
    "* We would like to build an optimal model to predict `Price` from the given feature variables\n",
    "* We would like to use our model to predict the price of other Italian restaurants\n",
    "\n",
    "Let's get our data into the environment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa4054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## assessing performance for ML models\n",
    "\n",
    "Last week we explored a few statistical approaches to modeling data.  \n",
    "For Machine Learning the goal is slightly different than Statistical Learning:  \n",
    "\n",
    "**Statistical Learning** - use our model to draw statistical inferences about the system  \n",
    "**Machine Learning** - emphasizes using the model to predict new cases  \n",
    "\n",
    "We would like to be able to assess our models ability to predict new observations.  \n",
    "To do this, we need divide our dataset into 2 sets:  \n",
    "\n",
    "* **Train** - data that will be used to train our model \n",
    "* **Test** - hold-out data to assess model performance. We will keep this data out of use until our model is developed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be905493",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## split Train/Test data sets with `sklearn`\n",
    "\n",
    "* X,y = predictors and Target\n",
    "* test_size - arbitrary, but typically an 80/20 train/test split\n",
    "* set a random state so our results are reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c8ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['Price', 'Case', 'Restaurant'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374ab26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will only use the test data splits once we are ready to test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ff1c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Multiple Linear Regression model\n",
    "\n",
    "* an initial EDA\n",
    "* we'll start simple, then build to kitchen sink model\n",
    "* model diagnostics\n",
    "* model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57767991",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.hist( bins = 20, figsize=(15,6), layout = (2,2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6601f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Looking for relationships\n",
    "\n",
    "Visualize relationships of the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644f119",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot( X_train )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d413dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corrMatrix = X_train.corr()\n",
    "plt.subplots( figsize = (10,8) )\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd2802",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are definitely relations between the numeric features (let's ignore 'East' for now).  \n",
    "A Correlation of 0.5 translates to 25% shared variance.  \n",
    "\n",
    "We will keep this in mind when we evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9966c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### using `statsmodels` to build up to a kitchen sink\n",
    "\n",
    "`statsmodel` is better for a pythonic approach to statistical learning. We will start with this \n",
    "\n",
    "But first we'll start with a simple linear regression of the dataset: `Price` modelled by `Food`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd504b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat( [X_train, y_train], axis=1 )\n",
    "simp_mod = smf.ols( formula = 'Price ~ Food', data = train)\n",
    "fitted_simp = simp_mod.fit()\n",
    "fitted_simp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336ca97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the fit\n",
    "fitted_simp.fittedvalues\n",
    "fig, ax = plt.subplots( figsize = (10,8) )\n",
    "ax.scatter( x = train['Food'], y = train['Price'], color = 'blue')\n",
    "ax.plot( train['Food'], fitted_simp.fittedvalues, 'k' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554aa4da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding the `East` categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a528b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simpE_mod = smf.ols( formula = 'Price ~ Food  + C(East)', data = train)\n",
    "fitted_simpE = simpE_mod.fit()\n",
    "fitted_simpE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31558b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding `Decor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b86473",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpD_mod = smf.ols( formula = 'Price ~ Food  + Decor', data = train)\n",
    "fitted_simpD = simpD_mod.fit()\n",
    "fitted_simpD.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38c9d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize as a plane in 3D\n",
    "x = np.linspace(16,25,10)\n",
    "y = np.linspace(14,24,10)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = (-25.9537 + 1.6773*X + 1.9376*Y) \n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure(figsize = (10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter( train['Food'], train['Decor'], train['Price'])\n",
    "surf = ax.plot_surface(X, Y, Z, alpha=0.5)\n",
    "ax.view_init(30, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9b711",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Kitchen Sink\n",
    "\n",
    "That's interesting, let's just see what happens when we include all variables in the model  \n",
    "What can we conclude from the model coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a full 'kitchen sink model'\n",
    "mlr_mod = smf.ols( formula = 'Price ~ Food + Decor + Service + C(East)', data = train)\n",
    "fitted_mlr = mlr_mod.fit()\n",
    "fitted_mlr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488e09e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostic Plots! \n",
    "\n",
    "Yep, diagnostic plots are still relevant for multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import scipy.stats as stats\n",
    "\n",
    "def linear_regression(df, x_cols, y_cols):\n",
    "    mod = sm.OLS(endog=df[y_cols], exog=df[x_cols]).fit()\n",
    "    influence = mod.get_influence()\n",
    "\n",
    "    res = df.copy()\n",
    "    res['resid'] = mod.resid\n",
    "    res['fittedvalues'] = mod.fittedvalues\n",
    "    res['resid_std'] = mod.resid_pearson\n",
    "    res['sqrt_resid_std'] = res['resid_std'].abs().transform('sqrt')\n",
    "    res['leverage'] = influence.hat_matrix_diag\n",
    "    res['norm_resid'] = mod.get_influence().resid_studentized_internal\n",
    "    res['cooks'] = influence.cooks_distance[0]\n",
    "    res['cooks_pval'] = influence.cooks_distance[1]\n",
    "    return mod, res\n",
    "\n",
    "\n",
    "def plot_diagnosis(df):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize = (10,8))\n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    # Residual against fitted values.\n",
    "    smooth = lowess( endog = df.resid, exog =  df.fittedvalues)\n",
    "    index, data = np.transpose(smooth)\n",
    "    axs[0,0].scatter( x = df.fittedvalues, y = df.resid, ls = 'None' )\n",
    "    axs[0,0].plot( index, data, 'r' )\n",
    "    axs[0,0].axhline( y=0, color='k')\n",
    "    axs[0,0].set_ylabel( 'Residual' )\n",
    "    axs[0,0].set_xlabel( 'fitted' )\n",
    "\n",
    "    # qqplot\n",
    "    sm.qqplot(\n",
    "        df['norm_resid'], dist=stats.t, fit=True, line='45',\n",
    "        ax=axs[0, 1]#, c='#4C72B0'\n",
    "    )\n",
    "    axs[0,1].set_title('Normal Q-Q')\n",
    "\n",
    "    # The scale-location plot.\n",
    "    smooth = lowess( endog = df.sqrt_resid_std, exog =  df.fittedvalues)\n",
    "    index, data = np.transpose(smooth)\n",
    "    axs[1,0].scatter(\n",
    "        x=df.fittedvalues, y=df.sqrt_resid_std\n",
    "    )\n",
    "    axs[1,0].plot( index, data, 'r' )\n",
    "    axs[1,0].set_xlabel('Fitted values')\n",
    "    axs[1,0].set_ylabel('Sqrt(|standardized residuals|)')\n",
    "    axs[1,0].set_title('Scale-Location')\n",
    "\n",
    "    # Standardized residuals vs. leverage\n",
    "    smooth = lowess( endog = df.resid_std, exog =  df.leverage)\n",
    "    index, data = np.transpose(smooth)\n",
    "    axs[1,1].scatter(\n",
    "        x=df.leverage, y=df.resid_std#, ax=axes[1, 1]\n",
    "    )\n",
    "    axs[1,1].axhline(y=0, color='grey', linestyle='dashed')\n",
    "    axs[1,1].plot( index, data, 'r' )\n",
    "    axs[1,1].set_xlabel('Leverage')\n",
    "    axs[1,1].set_ylabel('standardized residuals')\n",
    "    axs[1,1].set_title('Residuals vs Leverage')\n",
    "    leverage_top_3 = np.flip(np.argsort(df.cooks), 0)[:3]\n",
    "    for i in leverage_top_3:\n",
    "        axs[1,1].annotate(i, xy=(df.leverage[i],\n",
    "                                 df.norm_resid[i]));\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a375627",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mod_kitchensink, res = linear_regression( df=train, x_cols=['Food', 'Decor', 'Service', 'East'], y_cols=[\"Price\"] )\n",
    "plot_diagnosis( res )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f455d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicolinearity\n",
    "\n",
    "**Multicolinearity** - we have to consider that the predictors may not only be related to the target variable, but may be related among themselves.  \n",
    "\n",
    "**Statistical Learning** - multicolinearity does bad things to coefficient estimates (large std errors) and this makes interpretting the model difficult  \n",
    "**Machine Learning** - it can be shown that multicolinearity will not affect the predictions of the model\n",
    "\n",
    "Evaluate with the VIF measure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0caf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "\n",
    "vif_data['VIF'] = [variance_inflation_factor( X_train.values,i) for i in range( X_train.shape[1] ) ]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa685988",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use the Model to Predict Unseen data\n",
    "\n",
    "Predict on the X_test predictors evaluate on the y_test known outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_mlr.predict( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c9833",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DS4VS Happy Hour  üç∑ .....kinda\n",
    "\n",
    "Here is a toy dataset with several feature variables describing types of wine.  \n",
    "Try to implement a kitchen sink model with the data to predict the 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/bonzilla/Documents/ScienceLife/DS4VS/datasets/winequality_red.csv'\n",
    "wine = pd.read_csv( path, encoding= 'unicode_escape' )\n",
    "print( wine.head(), '\\n\\n' )\n",
    "print( wine.info() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516305f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e5dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9053cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/Week10/flowchart.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a295b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wonderful!, we just situated simple linear regression into the broader Linear Regression family. Next we will look at a completely different regression approach...\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
