{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a68ef52",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SmilodonCub/DS4VS/blob/master/Week11/DS4VS_week11_Supervised_Classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c454826",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 11 Introduction to Supervised Learning: Categorical Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79a9b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a Brief Recap:\n",
    "\n",
    "* Hello, how are you?\n",
    "* How are you with homework 3? Have you looked at Homework4?\n",
    "* A makeup date for SfN week.\n",
    "* Today: Supervised Learning Methods for Categorical Targets\n",
    "* Next 2 Weeks: Unsupervised Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07a43b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning for Categorical Data\n",
    "\n",
    "#### Fundamental Supervised Learning Classification Algorithms: \n",
    "\n",
    "* Logistic (& Multinomial) Logistic Regression\n",
    "* Support Vector Machines (SVM)\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Decision Trees & Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5be46a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will spend the next 2 hours taking a tour of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad22647",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "* [MNIST](http://yann.lecun.com/exdb/mnist/) is a classic machine learning dataset\n",
    "* a collection of labelled handwritten digits created in 1998\n",
    "* images are grayscale, centered, 28x28 \n",
    "* is made readily available from `scikit-learn`\n",
    "    - 60k training\n",
    "    - 10k test\n",
    "* subject of [numerous studies](https://paperswithcode.com/sota/image-classification-on-mnist) some with accuracies higher than human performance.\n",
    "    - most recent record: 0.18% error rate\n",
    "* has spawned many other datasts\n",
    "    - Fashion MNIST\n",
    "    - EMNIST\n",
    "    - and [many others](https://www.kaggle.com/datasets?search=MNIST&datasetsOnly=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0fb24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take a look!......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb89374",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bringing MNIST into our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552854a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a53ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "print( type( mnist ) )\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c765e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Getting to know MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print( 'X: ', X.shape, '\\ny: ', y.shape )\n",
    "print( 'some ys: ', y.iloc[0:10].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most scikit-learn algos are expecting a numeric classification\n",
    "y = y.astype( np.uint8 )\n",
    "print( 'some ys: ', y.iloc[0:10].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e824632",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize digit images\n",
    "fig = plt.subplots(figsize=(15,12))\n",
    "plt.subplots_adjust( hspace=0.8 )\n",
    "a = 5  # number of rows\n",
    "b = 5  # number of columns\n",
    "\n",
    "for character in range(0,a*b):\n",
    "    some_digit = np.array( X.iloc[character] )\n",
    "    plt.subplot( a,b,character + 1 )\n",
    "    some_digit_image = some_digit.reshape( 28, 28 )\n",
    "    plt.imshow( some_digit_image, cmap = \"binary\" )\n",
    "    plt.title('label: %i\\n' % y.iloc[character], fontsize = 14)\n",
    "    plt.axis( 'off' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c33803",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# functionalize this plot for future use\n",
    "def plot_digits( images, index_list, images_mat_dim=10):\n",
    "    a = images_mat_dim  # number of rows\n",
    "    b = images_mat_dim  # number of columns\n",
    "\n",
    "    for idx, im_idx in enumerate( index_list ):\n",
    "        some_digit = np.array( images.iloc[im_idx] )\n",
    "        plt.subplot( a,b,idx + 1 )\n",
    "        some_digit_image = some_digit.reshape( 28, 28 )\n",
    "        plt.imshow( some_digit_image, cmap = \"binary\" )\n",
    "        plt.axis( 'off' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_size = 5\n",
    "plot_digits( X, list( range( 0, mat_size*mat_size ) ), mat_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdced63",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39efda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll work with the training set to train our model.  \n",
    "Once we develop our model, we can evaluate with the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a3f0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Classification\n",
    "\n",
    "Predicting a classification where only two outcomes are possible. \n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Uses a logistic function to model a binary target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7d9bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remembering the Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1a4e0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "t = np.linspace(- 10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\") \n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\") \n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69fe74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Logistic Function: $$\\sigma(t) = \\frac{e^t}{e^t+1} = \\frac{1}{1 + e^{-t}}$$\n",
    "\n",
    "* We assume a Linear relationship for our classification problem:  $$t = \\beta_0 + \\beta_1x + \\cdots$$\n",
    "\n",
    "* We can rewrite our Logistic Regression Function as: $$\\sigma(t) = \\frac{e^t}{e^t+1} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x + \\cdots)}}$$\n",
    "\n",
    "* fit to our data (e.g. Gradient Descent methods) such that we minimize a cost function.\n",
    "\n",
    "[a casual explanation](https://towardsdatascience.com/whats-linear-about-logistic-regression-7c879eb806ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff53fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rephase MNIST for Binary Logistic Regression\n",
    "\n",
    "Let's build a binary logistic classifier to determine if a given digit is itself or not.  \n",
    "[`scikit-learn` docs](https://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 5\n",
    "y_train_exp = (y_train == exp) # True where y==exp ; False everywhere else\n",
    "y_test_exp = (y_test == exp) # \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate a Logistic Regression class object\n",
    "log_reg = LogisticRegression(penalty='l1', solver='saga', tol=0.1) # lbfgs is a faster solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to the data\n",
    "log_reg.fit(X_train, y_train_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c83e47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression()` predictions\n",
    "\n",
    "evaluate the model's fit to the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d41d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( log_reg.predict(X_train[0:10]) )\n",
    "print( y_train_exp.iloc[0:10].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(log_reg, X_train, y_train_exp, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b276c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `cross_val_score`\n",
    "\n",
    "`cross_val_score` - evaluate a score by cross-validation\n",
    "\n",
    "* **scoring** - [model selection and evaluation tools](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "* **cross-validation** - a resampling method. a portion of the training data is held out from model training and used to estimate model accuracy during model development.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588106c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating a Classifier\n",
    "\n",
    "Questions: What would chance performance be here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5444a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dummy Classifier** - a type of classifier that does not generate any insight about the data and classifies by a simple rule without trying to find a pattern in the data. For example, classify everything as the most common class.\n",
    "\n",
    "What **accuracy** do you expect from a classifier that classifies everything as `False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18305ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "`cross_val_predict` - generate a set of predictions. the cross-validation means that for each 'fold', the model was fit on the remainder of the data and returns a set of predictions on the unseen fold.  \n",
    "\n",
    "**confusion matrix**  \n",
    "\n",
    "|                  |   **Predicted True**  |    **Predicted False**   |\n",
    "|:----------------:|:---------------------:|:------------------------:|\n",
    "|  **Actual True** | true positives (TP)  | false negatives (FN)  |\n",
    "| **Actual False** |    false positives (FP)    |      true negatives (TN)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8223ab9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What would the confusion matrix for a perfect classifier look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb56f77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Confusion matrix for our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(log_reg, X_train, y_train_exp, cv=5)\n",
    "confusion_matrix(y_train_exp, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6922ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision & Recall\n",
    "\n",
    "* **precision** - what proportion of actual positives get correctly labeled as such? $\\frac{TP}{TP+FP}$\n",
    "\n",
    "* **sensitivity** (recall) - what proportion of predicted positives are actual positives? $\\frac{TP}{TP+FN}$\n",
    "\n",
    "* **$F_1 \\mbox{score}$** we can combine these precision and recall as a metric to evaluate our model. a harmonic mean: will only get a high score if both precision and recall are high. $\\frac{\\mbox{precision}*\\mbox{recall}}{\\mbox{precision}+\\mbox{recall}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print( 'precision: ', precision_score(y_train_exp, y_train_pred) )\n",
    "print( 'recall: ', recall_score(y_train_exp, y_train_pred) )\n",
    "print( 'f1_score: ', f1_score( y_train_exp, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fb7bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where is the Model Getting Confused?\n",
    "\n",
    "Let's visualize some of the mislabeled digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_T, cl_F = True, False\n",
    "X_TT = X_train[(y_train_exp == cl_T) & (y_train_pred == cl_T)]\n",
    "X_TF = X_train[(y_train_exp == cl_T) & (y_train_pred == cl_F)]\n",
    "X_FT = X_train[(y_train_exp == cl_F) & (y_train_pred == cl_T)]\n",
    "X_FF = X_train[(y_train_exp == cl_F) & (y_train_pred == cl_F)]\n",
    "mat_size = 5\n",
    "\n",
    "plot_digits( X_TT, list( range( 0, mat_size*mat_size ) ), mat_size)  #True Positives\n",
    "#plot_digits( X_TF, list( range( 0, mat_size*mat_size ) ), mat_size)  #False Negative \n",
    "#plot_digits( X_FT, list( range( 0, mat_size*mat_size ) ), mat_size)  #False Positive\n",
    "#plot_digits( X_FF, list( range( 0, mat_size*mat_size ) ), mat_size)  #True Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4d692",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could visually evaluate model performance by plotting $$\\mbox{precision} \\sim \\mbox{recall}$$  \n",
    "\n",
    "However, it is more common to evaluate $$\\mbox{true positive rate} \\sim \\mbox{false positive rate} == \\mbox{recall} \\sim 1-\\mbox{sensitivity}$$\n",
    "...in other words, the **ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886df817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "y_scores = cross_val_predict(log_reg, X_train, y_train_exp, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "log_reg_fpr, log_reg_tpr, thresholds = roc_curve( y_train_exp, y_scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa8f74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( log_reg_fpr, log_reg_tpr, linewidth = 2 )\n",
    "plt.plot( [0,1], [0,1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])                                  \n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate')   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ab934",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Area Under the Curve\n",
    "\n",
    "**Comparing Models** - the Area Under the Curve (AUC) is often used as a measure to compare classifier performance. A perfect classifier will have and AUC = 1.  \n",
    "\n",
    "What would be the AUC of a model performing at chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_exp, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1114e88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Machines \n",
    "\n",
    "* a simple algorithm that finds a **decision boundary** to separate target classes\n",
    "* decision boundary: classification will be assigned depending on what side of the boundary an observation appears\n",
    "* goal: maximize the distance between the nearest instances for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168f84e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The SVM decision boundary\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/809/1*GPFxwsE4cqcPxul4GWGp1A.png\" width=\"80%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "Let's see SVM in action with [this demo](https://jgreitemann.github.io/svm-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48d565",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting an SVM model to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364749ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform( X_train )\n",
    "X_test_scaled = scaler.transform( X_test )\n",
    "#svm_clf = SVC( kernel='linear', gamma='auto', probability=False )\n",
    "#svm_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970067e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SVM does not scale well in high dimentions.\n",
    "\n",
    "training will take very long time.  \n",
    "If you would like to try, uncomment the code below and train your own SVM.  \n",
    "There is an interesting new python library that will let you play Galaga while you model trains: [`TrainInvaders`](https://github.com/aporia-ai/TrainInvaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainInvaders\n",
    "# install the library\n",
    "#!pip3 install train_invaders --upgrade\n",
    "# import TrainInvaders\n",
    "import train_invaders.start\n",
    "# it will let you play until your model is done training.\n",
    "# you can turn it off:\n",
    "#import train_invaders.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba6bab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# if you would like to train your own SVM\n",
    "\n",
    "#svm_clf.fit( X_train_scaled, y_train_exp )\n",
    "# but this will take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10674ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load a pre-trained model\n",
    "\n",
    "import pickle\n",
    "filename = 'svm_clf'\n",
    "#pickle.dump(svm_clf, open(filename, 'wb'))  # to 'pickle' a Python object\n",
    "\n",
    "# load the model from disk\n",
    "svm_clf = pickle.load(open(filename, 'rb'))\n",
    "svm_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee822b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluating Model Fit to the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a50d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_svm = svm_clf.predict(X_train)\n",
    "confusion_matrix(y_train_exp, y_train_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5256aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'precision: ', precision_score(y_train_exp, y_train_pred_svm) )\n",
    "print( 'recall: ', recall_score(y_train_exp, y_train_pred_svm) )\n",
    "print( 'f1_score: ', f1_score( y_train_exp, y_train_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba068b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b73c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_y_scores = svm_clf.decision_function( X_train )\n",
    "svm_clf_fpr, svm_clf_tpr, svm_thresholds = roc_curve( y_train_exp, svm_y_scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e13501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the ROCs for both approaches\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( log_reg_fpr, log_reg_tpr, linewidth = 2, color = 'red' )\n",
    "plt.plot( svm_clf_fpr, svm_clf_tpr, linewidth = 2, color = 'green' )\n",
    "plt.plot( [0,1], [0,1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])                                  \n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate')   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df22e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compare AUCs for both approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7892f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'logistic AUC: ', roc_auc_score(y_train_exp, y_scores) )\n",
    "print( 'SVM AUC: ', roc_auc_score(y_train_exp, svm_y_scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228c557",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SVM has hyperparameters\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYG48xX8xCzQZ-EdDa_gfFjGGVY3onfwIB_A&usqp=CAU\" width=\"40%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd8ec9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training SVM\n",
    "\n",
    "* **C**\n",
    "* **gamma**\n",
    "\n",
    "There are also different the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#steps = [('scaler', StandardScaler()), ('SVM', SVC(kernel='linear'))]\n",
    "#pipeline = Pipeline(steps)\n",
    "#parameters = {'SVM__C':[0.001, 0.1, 100, 10e5], 'SVM__gamma':[10,1,0.1,0.01]}\n",
    "\n",
    "#grid = GridSearchCV(pipeline, param_grid=parameters, cv=5)\n",
    "#grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b27159",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://static.tvtropes.org/pmwiki/pub/images/intermission_3696.jpg\" width=\"70%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba1edb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial Classification\n",
    "\n",
    "distinguishing between more than two classes\n",
    "\n",
    "* Logistic Regression and SVM are stricktly binary classifiers.\n",
    "* There are methods to adapt a Multinomial Logistic Regression and SVM for more than two classes\n",
    "    - **OvR** - one-vs-the-rest\n",
    "    - **OvO** - one-vs-one\n",
    "* There are other classification algorithms that can directly handle more than 2 classes:  \n",
    "    - kNN, naive Bayes, Decision Trees, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ab419",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial Logistic & SVM Classification\n",
    "\n",
    "we are going to quickly train multinomial versions of the Logistic & SVM models that we built for the binary case above.  \n",
    "The Python code looks remarkably similar, because `scikit-learn` detects the number of classes to adjust the algorithm. However, conceptually we are building very different classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regression\n",
    "mlr_mod = LogisticRegression(penalty='l1', solver='saga', tol=0.1)\n",
    "mlr_mod.fit(X_train, y_train) # not y_train_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b19985",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mlr_y_train_pred = cross_val_predict(mlr_mod, X_train, y_train, cv=5) #not y_train_exp\n",
    "mlr_conf_mat = confusion_matrix(y_train, mlr_y_train_pred)\n",
    "print( mlr_conf_mat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88cbad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "row_sums = mlr_conf_mat.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = mlr_conf_mat / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray, fignum=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5450f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print( classification_report( y_train, mlr_y_train_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SVM multinomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial SVM\n",
    "#multisvm_clf = SVC( kernel='linear', gamma='auto', probability=False )\n",
    "#multisvm_clf.fit( X_train_scaled, y_train ) #not y_train_exp\n",
    "# you can do this if you really want to ....or if you just want to play more TrainInvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78812853",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'multisvm_clf'\n",
    "#pickle.dump(multisvm_clf, open(filename, 'wb'))  # to 'pickle' a Python object\n",
    "\n",
    "# load the model from disk\n",
    "multisvm_clf = pickle.load(open(filename, 'rb'))\n",
    "multisvm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a84308",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "msvm_y_train_pred = multisvm_clf.predict(X_train) #not y_train_exp\n",
    "msvm_conf_mat = confusion_matrix(y_train, msvm_y_train_pred)\n",
    "print( msvm_conf_mat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53a23d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "row_sums = msvm_conf_mat.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = msvm_conf_mat / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray, fignum=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56b16c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( classification_report( y_train, msvm_y_train_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03de1c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## k-Nearest Neighbors (kNN)\n",
    "\n",
    "classify an observation based on the known labels of the nearest neighbors \n",
    "\n",
    "<img src=\"https://vitalflux.com/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-2.34.57-PM.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d2e98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### kNN pseudocoded\n",
    "\n",
    "kNN is a very simple algorithm!\n",
    "\n",
    "* for each training data point:  \n",
    "    - calculate the distance between all other training data\n",
    "        * what is 'distance': Euclidean, cosine, Chebyshev, Manhattan\n",
    "    - sort by increasing distance\n",
    "    - use the 'k' closest points to determine\n",
    "        * winner-takes-all, or a weighted distance etc.\n",
    "        \n",
    "There are many resouces that walk through building up [kNN from scratch](https://www.youtube.com/watch?v=n3RqsMz3-0A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ced883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9ea71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# retrun the number of neighbors used by our classifier\n",
    "knn_clf.n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebf5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning 'k'\n",
    "\n",
    "How do we determine the best value of 'k'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_params = {'n_neighbors': list( range( 1, 25, 2 ) ) }\n",
    "grid_knn = GridSearchCV( knn_clf, grid_params, cv = 3 )\n",
    "knn_grid_res = grid_knn.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a03034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'knn_grid'\n",
    "#pickle.dump(knn_grid_res, open(filename, 'wb'))  # to 'pickle' a Python object\n",
    "\n",
    "# load the model from disk\n",
    "#knn_grid = pickle.load(open(filename, 'rb'))\n",
    "#knn_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46b08d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "knn_grid_res.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mnist_knn = knn_grid_res.best_estimator_\n",
    "\n",
    "#knn_y_train_pred = best_mnist_knn.predict(X_train) #not y_train_exp\n",
    "knn_y_train_pred = cross_val_predict(best_mnist_knn, X_train, y_train, cv=5) #not y_train_exp\n",
    "knn_conf_mat = confusion_matrix(y_train, knn_y_train_pred)\n",
    "print( knn_conf_mat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27392b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "row_sums = knn_conf_mat.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = knn_conf_mat / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray, fignum=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0db4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( classification_report( y_train, knn_y_train_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741dac6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Similar to kNN, Decision Trees are another non-parametric algorithm.  \n",
    "However, the classification approach is very different:  \n",
    "\n",
    "* Decision trees split the data on a measure of purity\n",
    "* `sklearn.DecisionTreeClassifier()` uses a common approach: CART\n",
    "* **CART** - Classification and Regression Tree\n",
    "    - uses the **gini** index as a purity measure\n",
    "    - a greedy algorithm - makes the locally optimal choice at each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69648c7e",
   "metadata": {},
   "source": [
    "### An MNIST illustration\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Soham-Saha-2/publication/329318295/figure/fig1/AS:792545151946752@1565968897034/The-hierarchy-tree-for-MNIST-learned-by-our-proposed-Class2Str-network-and-Latent.png\" width=\"70%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4039e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10ed3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "tree.plot_tree(tree_clf, max_depth=4, fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda771c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluate the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_y_train_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5) #not y_train_exp\n",
    "tree_conf_mat = confusion_matrix(y_train, tree_y_train_pred)\n",
    "print( tree_conf_mat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = tree_conf_mat.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = tree_conf_mat / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray, fignum=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( classification_report( y_train, tree_y_train_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447b361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which Classification Method to use?\n",
    "\n",
    "which model are we most satisfied with and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba8e15",
   "metadata": {},
   "source": [
    "## Python & Pandas 4 Penguins\n",
    "\n",
    "Let's use an example dataset to learn about multinomial logistic regression.  \n",
    "We will be using the [Palmer Penguins](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081) [dataset](https://github.com/allisonhorst/palmerpenguins). It's a newer alternative to the [classic Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c81eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/datasets/penguins.csv\")\n",
    "\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c6541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1e48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e8002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da61bb3f",
   "metadata": {},
   "source": [
    "## Take your time to dive into each classification method with the relatively simple Palmers Penguins dataset\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
