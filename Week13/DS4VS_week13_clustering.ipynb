{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff267040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 13: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a7ace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief Recap:\n",
    "\n",
    "* Hello, how are you?\n",
    "* Today: Clustering!\n",
    "* Next Class (this Friday): an Introductory Guide to Neural Networks\n",
    "* After Thanksgiving: Presentations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf553e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "**Clustering** - the task of identifying similar instances in the data and assigning them to clusters by giving them new class labels. Clusters are subgroupings of similar data.  \n",
    "\n",
    "Clustering is an unsupervised learning method with many applications:  \n",
    "\n",
    "* anomoly detection\n",
    "* customer segmentation (for recommender systems)\n",
    "* image segmentation\n",
    "* search engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7d808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture Densities\n",
    "\n",
    "Before we dive into Clustering, let's discuss the concept of **Mixture Densities**  \n",
    "\n",
    "**Mixture Densities** - complicated populations of data sampes that can be expressed as a combination of much simpler subpopulations. e.g. a mixture of gaussians\n",
    "\n",
    "Let's visualize a mixture of gaussians..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0139f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in some necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15afc14",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "cluster_stds = np.random.uniform(low=2, high=5, size=(n_components,))\n",
    "X, blob_label = make_blobs(n_samples=300, centers=n_components, \n",
    "                      cluster_std = cluster_stds, \n",
    "                      random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = blob_label)\n",
    "plt.title(f\"Example of a mixture of {n_components} distributions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc6e49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More on Mixture Densities\n",
    "\n",
    "A mixture density is described mathematically as: $p(\\mathbf{x}) = \\sum^{k}_{i=1}p(\\mathbf{x}|\\mathcal{G}_i)P(\\mathcal{G}_i)$\n",
    "\n",
    "$\\mathcal{G}_i$ refers to each mixture component (group). We can break this mess of math down as:  \n",
    "\n",
    "The Mixture Density ($p(\\mathbf{x})$) is given by the sum from $1 \\rightarrow k$ ($k =$ number of components) of the product of the density of each component ($p(\\mathbf{x}|\\mathcal{G}_i)$) and the mixture proportion of that component ($P(\\mathcal{G}_i)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6b466",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we assume our system is a mixture of gaussians (as in the python example we plotted),  \n",
    "\n",
    "* $p(\\mathbf{x}|\\mathcal{G}_i) \\sim \\mathcal{N}(\\mathbf{\\mu}_i, \\sum _i))$\n",
    "* parameters to estimate: weights, means & covariance mats for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67370a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification application of Misture Densities\n",
    "\n",
    "we can rephrase mixture densities for classification:  \n",
    "\n",
    "* Groups $\\sim$ Classes\n",
    "* Component Densities $\\sim$ Class Densities\n",
    "* Component Priors $\\sim$ Class Priors\n",
    "\n",
    "We can rewrite: $p(\\mathbf{x}) = \\sum^{K}_{i=1}p(\\mathbf{x}|\\mathcal{C}_i)P(\\mathcal{C}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc904b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Mixture Densities\n",
    "\n",
    "In the supervised case, we are given the labels along with the observed value. Our task is to learn the parameters that describe the distributions: priors, means & standard deviations \n",
    "\n",
    "For a Gaussian Mixture the parameters can be learned with [maximum likelihood](https://courses.cs.duke.edu/spring04/cps196.1/handouts/EM/tomasiEM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4306d16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Mixture Densities\n",
    "\n",
    "For the Unsupervised learning problem, we only have the observed value; the labels are unkown.  \n",
    "What to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(f\"Unsupervised mixture of {n_components} distributions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74963456",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning of Class Membership\n",
    "\n",
    "**Clustering** - the task of identifying similar instances and assigning them to clusters, or groups of similar instances.\n",
    "\n",
    "To properly describe this mixture system into clusters, we need a different approach to the problem:\n",
    "\n",
    "1. Estimate Class Labels\n",
    "2. Estimate the Parameters of the Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57107ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating Class Labels using k-Means Custering\n",
    "\n",
    "k-Means is a simple algorithm for assigning class labels to data points.  \n",
    "k-Means pseudocoded:  \n",
    "\n",
    "    Initialize means as 'k' random values\n",
    "    \n",
    "    For a maximum number of iterations:  \n",
    "        For each data point: \n",
    "            find the distance between the data point and the k means\n",
    "            find the closest mean\n",
    "            assign the datapoint to the closest mean\n",
    "        Update the k mean values as the mean of all assigned datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4989c98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Naive k-Means\n",
    "\n",
    "We initialize 'k' means randomly from within a given range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280d401",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "but there are other options that can behave more optimally  \n",
    "\n",
    "* **domain knowledge** - can specify wiht domain knowledge if available\n",
    "* **Random Partition** - randomly assign each observation to one of the 'k', determine the k-means, and iterate from there\n",
    "* **k-means++** - chose the 1st k randomly from the data points. chose the next ks with a probability proportional to the distance of the neared k that has already been assigned\n",
    "\n",
    "`sklearn` will use k-means++ by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89205b55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Determining distance for the k-Means algorithm\n",
    "\n",
    "* **Euclidean** currently, `sklearn` only support euclidean distance k-means\n",
    "* **Cosine** cosine of the vector angle\n",
    "* **Manhattan** sum of distance in all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fdf01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### k-Means Clustering decision\n",
    "\n",
    "* **hard clustering** - the k-means algorithm assigns class membership for each observation to one and only one class  \n",
    "    - estimated label = $\\left\\{ \\begin{array}{rcl}\n",
    "1 & \\mbox{if} & \\Vert x^t-m_i\\Vert = \\mbox{min}_j \\Vert x^t-m_j \\Vert \\\\ \n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array}\\right.$\n",
    "    - This can be problematic for observations near class borders.  \n",
    "* **soft clustering**  algorithms that instead return class membership probability or weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9604a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating the Clusters' Parameters \n",
    "\n",
    "Once we have estimated the class labels, we can estimate the parameters of each class.  \n",
    "For example, if we assume a mixture of gaussian distributions, it is straight forward to estimate:  \n",
    "\n",
    "* $P(\\mathcal{C}_i)$ - the Class priors \n",
    "* $m_i$ - estimate for the mean\n",
    "* $S_i$ - estimate for the covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603f8e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use `sklearn` to perform k-Means\n",
    "\n",
    "Of course, if you are curious, there are many resources online to guide you through an [implementation of k-means from scratch](https://towardsdatascience.com/k-means-clustering-from-scratch-6a9d19cafc25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans( n_clusters = n_components, random_state=0 ).fit( X )\n",
    "print( kmeans.cluster_centers_ )\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dc1a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results of the kMeans fit\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12,6))\n",
    "ax2.scatter(X[:, 0], X[:, 1], s=50, c = kmeans.labels_)\n",
    "ax2.set_title(\"k-Means results\")\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=50, c = blob_label)\n",
    "ax1.set_title(f\"Example of a mixture of {n_components} distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe87a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voronoi Diagram\n",
    "\n",
    "also called Voronoi Tesselations/Decompositions/Partitions or Dirichlet Tesselations.  \n",
    "divides the data into Voronoi cells, or Theissian polygons.  \n",
    "\n",
    "**Voronoi Cell** - each cell has a seed. the cell consists of all data points which are closer to the cell's seed than any other seed. Note that we can use different approaches to define the seed or to determine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(clusterer, X, resolution=1000,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    \"\"\"\n",
    "    adapted from Hands-on Machine Learning (Geron 2nd edition)\n",
    "    \"\"\"\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33035e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = kmeans.labels_)\n",
    "plt.title(f\"Kmeans (k= {n_components} ) with Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2753c05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Making predictions with new observations\n",
    "\n",
    "with a trained k-means model, we can easily give class assignments to new observations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf95c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Observations = np.array([[-15,-5],[10,0],[-5,10]])\n",
    "kmeans.predict(New_Observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bbca7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# we can also learn the centroids kmeans found\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6143ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The previous Voronoi figure illustrated the Class labels determined by k-means.  \n",
    "K-means is an unsupervised method. However, in this toy data example, we happen to know the ground truth labels for each data point.  \n",
    "\n",
    "Let's observe the ground truth against the decision boundaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647f993",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = blob_label)\n",
    "plt.title(f'Kmeans (k={n_components}) with Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353b825",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now you try!\n",
    "\n",
    "Let's pause here.  \n",
    "Take some time to review the previous code and make some changes. For example:  \n",
    "\n",
    "* change the `n_components`\n",
    "* change the cluster_stds low & high parameters\n",
    "\n",
    "How well does K-means perform when you increase 'k'? or change the spread of the data blobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c307cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the best 'k'?\n",
    "\n",
    "Notice how in the previous examples, we fit a k-means model with a 'k' value that was determined by the number of gaussian 'blobs' we added to the toy data set.  \n",
    "\n",
    "In the real world, when we use k-means, we might not have the luxury of knowing how many 'blobs' we have.  \n",
    "\n",
    "In fact, 'k' is a hyperparameter that we need to tune and doing so may find a less biased pattern in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444c337",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clustering with the Titanic data set\n",
    "\n",
    "The Titanic dataset is a popular one. It holds features on passenger information for every passenger listed aboard the Titanic including: age, sex, name, ticket class, fare and even the cabin number to be used for predicting passenger survival (a supervised classification problem).  \n",
    "\n",
    "Here we will just use a couple of features (passenger age and fare price) to perform a k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic = pd.read_csv( url )\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81c9d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Light Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = titanic[['Fare','Age','Survived']].copy()\n",
    "cluster_df.dropna(axis=0, inplace=True)\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e932f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a140c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(cluster_df['Fare'], cluster_df['Age'], s=50, c = cluster_df['Survived'])\n",
    "plt.title('Titanic Passenger Data: Fare ~ Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56069b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inertia\n",
    "\n",
    "**Inertia** - a metric the measures the distance between each instance and its centroid.  \n",
    "\n",
    "Inertia can help evaluate a k-means model at a given 'k' values.  \n",
    "However, inertia will necessarily decrease with increasing 'k', as we will see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb685e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(cluster_df[['Fare','Age']])\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "inertias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eebed3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What to do? ... the Inertia vs k plot\n",
    "\n",
    "If we plot inertia for each value of 'k' we can arrive at a good choice of 'k' by finding the 'elbow' in the curve this plot produces.  \n",
    "\n",
    "**The Elbow** - a lower 'k' would have a dramatically higher inertia, but an higher 'k' would not achieve much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.axis([0, 10, 0, 2000000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17842f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([0, 10, 0, 2000000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d27a6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Silhouette Diagrams\n",
    "\n",
    "Did finding the 'elbow' feel uncomfortably subjective?  \n",
    "\n",
    "Another method is to find the 'k' values with the highest silhouette score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a917df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " **Silhouette Score** - mean silhouette coefficient for all instances  \n",
    "**Silhouette Coefficient** - mean distance to the other instances in the same cluster  \n",
    "$$\\mbox{Silhouette Coefficient} = \\frac{(b-1)}{\\mbox{max}(a,b)}$$\n",
    "where:  \n",
    "\n",
    "* a = intra-cluster distance, the mean distance to other instances in the same cluster\n",
    "* b = mean distance to instances in the nearest cluster\n",
    "\n",
    "A Silhouette Coefficient ~1 means the instance is well within it's cluster  \n",
    "\"\" ~-1 means the instance might have been misassigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1f46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = [silhouette_score(cluster_df[['Fare','Age']], model.labels_) for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894de5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 9, 0.4, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1a2c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Silhouette Diagram\n",
    "\n",
    "The previous figure shows the mean silhouette score for all observations for each cluster.  \n",
    "However, we can infer a lot more with the Silhouette Diagram:  \n",
    "\n",
    "[**Silhouette Diagram**](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) - plot every instances's silhouette coefficient sorted by cluster membership and value of the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33813d",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from sklearn documentation\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "X = cluster_df[['Fare','Age']]\n",
    "\n",
    "for k in (2, 3, 4, 5, 6):\n",
    "    plt.subplot(2, 3, k - 1)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    \n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6452e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpretting the Silhouette Diagram\n",
    "\n",
    "The red dashed lines show the mean silhouette score for all observations for each 'k'.  \n",
    "Compare with the Silhouette Score plot to verify that they are indeed the same values.  \n",
    "\n",
    "If a cluster does not pass the mean silhouette score, then this is a poorly assigned cluster.  \n",
    "Our goal is to select a minimal value of 'k' where all cluster cross the mean score threshold.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8bc6c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which value of 'k' satisfies this criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8ba7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train a k-means model with k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "X['y_pred'] = y_pred\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3c5b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the Clustered Data with k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(X['Fare'], X['Age'], s=50, c = X['y_pred'])\n",
    "plt.title(f'Titanic Passenger Data: k={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fef325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering the Iris dataset\n",
    "\n",
    "Let's try a similar approach with the iris dataset using just the features for the petals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "subiris = iris_df[['petal length (cm)','petal width (cm)']].copy()\n",
    "subiris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b35ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(subiris['petal length (cm)'], subiris['petal width (cm)'], s=50)\n",
    "plt.xlabel(\"petal length (cm)\", fontsize=14)\n",
    "plt.ylabel(\"petal width (cm)\", fontsize=14)\n",
    "plt.title('Unlabeled Iris')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate & plot the inertias for various k\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(subiris) for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "print( inertias )\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.axis([0, 10, 0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c959a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which 'k'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(subiris)\n",
    "subiris['y_pred'] = y_pred\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(subiris['petal length (cm)'], subiris['petal width (cm)'], s=50, c = subiris['y_pred'])\n",
    "plt.title(f'Iris Petal Data: k={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0457d34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now you try!\n",
    "walk through the same code using only the iris 'sepal' features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1c44a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems with k-means\n",
    "\n",
    "* Depending on the initial centroid values, k-means might not reach the same values during training. It is often necessary to run k-means several times to find a solution\n",
    "* K-means does not behave well if the variances of different clusters are very different\n",
    "* K-means does not 'like' non-gaussian clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfb21c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ðŸ¥º The next lecture will be our last lecture class ðŸ˜¢\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
