{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd1888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d70dcfd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10: Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cf259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On Noise\n",
    "\n",
    "$$Y \\approx \\beta_0 + \\beta_1X + \\beta_2X + \\dots + \\beta_NX$$\n",
    "\n",
    "Linear Regression finds the input-output relationahip as a weighted sum of the predictors.  \n",
    "However, the data is not perfect.   \n",
    "There is necessarily error/noise present  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028d258",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A Multiple Linear Regression Phenomenon**  \n",
    "For a training given dataset, as more features are added to a model the $R^2$ increases even if the added parameter in uninformative.  \n",
    "At a certain point, adding new parameters fits the model to the noise inherent in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62c725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bias Variance Trade-off\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*1BGl9kfU6nwO2QQ0-fWHcg.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6334a73",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalization Error\n",
    "\n",
    "**Generalization Error** - a measure of how accurately a model can predict previously unseen data  \n",
    "\n",
    "Comparing measures generalization is informative of the optimal model complexity\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/0NbOY.png\" width=\"80%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9f2af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/max/875/0*XCe3mlLeGiUW3xfh\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b941b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization: bringing to uniformity\n",
    "\n",
    "**Regularized Linear Models**  \n",
    "\n",
    "* Regularize a model to reduce overfitting: constrain it somehow\n",
    "* For Linear Regression this means: constrain the weights (parameters) of the model. \n",
    "* This is usually implemented by adding a regularization term to the cost function\n",
    "\n",
    "Today we will survey regularization methods for linear models  \n",
    "\n",
    "1. Ridge Regression\n",
    "2. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e0dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meatspec Dataset\n",
    "\n",
    "Since determining the fat content via analytical chemistry is time consuming (and expensive), a company would like to build a model to predict the fat content of new samples using absorbance spectra data (which can be measured more easily and cheaper).\n",
    "\n",
    "**The Predictors** - 100 channels measure different near infrared absorpances.  \n",
    "**The Target** - a measure of fat content  \n",
    "**the shape** - (215,101) 215 records from meats with known fat content and data from 100 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abe2a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bring the data into the environment\n",
    "url = 'https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/datasets/meatspec.csv'\n",
    "df = pd.read_csv( url, index_col=0 )\n",
    "df.shape\n",
    "\n",
    "X = df.drop(['fat'], axis=1)\n",
    "y = df['fat']\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(data = X_scaled, columns = X.columns)\n",
    "\n",
    "X_scaled_train, X_scaled_test, y_train, y_test = train_test_split( X_scaled, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08fda6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use `sklearn` to build a 'kitchen sink' MLR\n",
    "\n",
    "we will use this both to see how MLR is done with `sklearn` and to compare performance with Regularization\n",
    "\n",
    "`LinearRegression()` will implement OLS. OLS is ideal when the underlying relationship is Linear and we have n>>p. But if n is not much larger than p or p>n (unfeasible for OLS), there can be a lot of variability in the fit which can result in either overfitting and very poor predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1379c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# instantiate a Linear Regression Model\n",
    "lin_mod = LinearRegression()\n",
    "# fit the model to the training data\n",
    "lin_mod.fit( X_scaled_train, y_train )\n",
    "# print the model intercept & coefficients\n",
    "print( lin_mod.intercept_, lin_mod.coef_ )\n",
    "# print the training R2 score\n",
    "score=r2_score(y_train,lin_mod.predict(X_scaled_train))\n",
    "print( 'r2 Training score is ', score )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9ba59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/Week10/overfitting.jpeg\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179101c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluate the Model Performance on unseen data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions on the test dataset\n",
    "y_prediction = lin_mod.predict(X_scaled_test)\n",
    "# predicting the accuracy score\n",
    "score=r2_score(y_test,y_prediction)\n",
    "print( 'Test data')\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,y_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,y_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ac733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1) Ridge Regression\n",
    "\n",
    "**Ridge Regression**  \n",
    "\n",
    "- add a term to the cost function that froces the model to minimize the model weights. \n",
    "- **Cost Function** $J(\\theta) = \\mbox{MSE}(\\theta) + \\alpha \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "- half the square of the $l_2$norm\n",
    "- **$\\alpha$** - a hyperparameter that controls the minimization\n",
    "    * $\\alpha$ == 0 is basically MLR\n",
    "    * $\\alpha$ is large, and the weights are close to zero (regress to bias) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2d29d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "ridge_reg.fit(X_scaled_train, y_train)\n",
    "print( ridge_reg.intercept_, ridge_reg.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e15b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# use the model to make predictions on the test dataset\n",
    "ridgey_prediction = ridge_reg.predict( X_scaled_test )\n",
    "# predicting the accuracy score\n",
    "score=r2_score(y_test,ridgey_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,ridgey_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,ridgey_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c3623",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### HHhhmmmmmm\n",
    "\n",
    "that $R^2$ score is an improvement. However, we just picked a totally random $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20f5dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter $\\alpha$\n",
    "\n",
    "We could just randomly try $\\alpha$s until we get a good result, but that would be inefficient and very biased.  \n",
    "`scikit-learn` will come to the rescue with the `GridSearchCV`  \n",
    "\n",
    "[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) will perform a cross-validated sweep of a parameter space to find the best value for $\\alpha$  \n",
    "How convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Ridge()\n",
    "# define the parameter space\n",
    "#parameters = {'alpha':[1, 2, 3, 4, 5, 10, 15, 20, 25, 50, 75, 100, 250, 500, 1000]}\n",
    "parameters = {'alpha':list(np.linspace(0.00001,.001, 1001))}\n",
    "# define the grid search\n",
    "Gridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Gridge_reg.fit(X_scaled_train,y_train)\n",
    "# best estimator\n",
    "print(Gridge_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f95dc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge Regression: Best Gridsearch Model\n",
    "\n",
    "Let's use our best $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "best_Gridge_mod = Gridge_reg.best_estimator_\n",
    "best_Gridge_mod.fit(X_scaled_train,y_train)\n",
    "print( best_Gridge_mod.intercept_, best_Gridge_mod.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Gridge_prediction = best_Gridge_mod.predict( X_scaled_test )\n",
    "score=r2_score(y_test,best_Gridge_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,best_Gridge_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,best_Gridge_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f91d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Lasso Regression\n",
    "\n",
    "**Least Absolute Shrinkage and Selection Operator Regression** - Similar to Ridge, but adds the $l_1$ norm to the cost function  \n",
    "\n",
    "- **Cost Function** $J(\\theta) = \\mbox{MSE}(\\theta) + \\alpha \\sum_{i=1}^n |\\theta_i|$\n",
    "- tends to eliminate weights of unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5445fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(tol=1e-2, max_iter=100000 )\n",
    "parameters = {'alpha':[0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 1, 2, 3, 4, 5, 10, 15, 20]}\n",
    "# define the grid search\n",
    "Glasso_reg= GridSearchCV(lasso_reg, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Glasso_reg.fit(X_scaled_train,y_train)\n",
    "# best estimator\n",
    "print(Glasso_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a30c29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso Regression: Best Gridsearch Model\n",
    "\n",
    "Let's use our best $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee516d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# best model\n",
    "best_Lasso_mod = Glasso_reg.best_estimator_\n",
    "best_Lasso_mod.fit(X_scaled_train,y_train)\n",
    "print( best_Lasso_mod.intercept_, best_Lasso_mod.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Lasso_prediction = best_Lasso_mod.predict( X_scaled_test )\n",
    "score=r2_score(y_test,best_Lasso_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,best_Lasso_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,best_Lasso_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e321dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week: Supervised Learning techniques for Categorical Target Variables\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
